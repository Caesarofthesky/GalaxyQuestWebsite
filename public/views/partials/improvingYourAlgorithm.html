<div>
  <div class="chapterTitle">
    <h2>Chapter 4: Optimizing our Machine and Travelling Into Space</h2>
  </div>
<div class="row featurette">
  <div class="col-md-5">
    <div class="chart">
      <div class="errorChart"></div>
    </div>
  </div>
  <div class="col-md-7">
      <h2 class="featurette-heading">Knowing Your Error</h2>
  </div>
  <p class="lead">Once you have your first result under your belt, now comes the task of improving the margin of error.</p>
</div>
<hr class="featurette-divider">
<div class="row">
  <h3 class="pull-left">Our heros' epic battle with error margins</h4><br><br><br><br>
      <ul>
        <li><h4 class="text-info">Playing with the algorithm's parameters</h4>
          <ul><li>The parameters that may be adjusted in a random forests algorithm include the max-depth of our trees, the number of trees we used, amount of features to use and the size of the subset of features each tree can choose from. We adjusted these parameters to try and find an optimal configuration for our problem.</ul></li>
        <li><h4 class="text-info">Create features</h4>
          <ul><li>We added additional input features to feed to our learning algorithm that we thought would decrease the generalization error observed once the algorithm was trained. Measuring generalization error for each of the different outputs we produce gave us an idea of what we could improve on. Sometimes adding features that seemed like they would help actually ended up increasing our generalization error. We hypothesize that this was the case because the new features may have been increasing our model's bias towards those features of the data.</ul></li>
        </li></li>
</div>

<hr class="featurette-divider">
<div class="row">

  <h3 class="pull-right">Our Learnings and Discovered Best Practices</h4><br><br><br><br>
      <ul>
        <li><h4 class="text-info">Avoid overfitting and underfitting</h4>
          <ul><li>Measuring training and test error of your current algorithm can help you to understand wether you are having a overfitting or underfitting problem. Both the algorithm's parameters and input features can affect overfitting and underfitting. Overfitting will give you a low margin of error on your training set, but will not produce a great result when you introduce new data to predict on. Underfitting will generally give you high margin of error across both training and new data. Use this information to understand what parameters need adjustment in your algorithm.</ul></li></li>

</div>
<script src="../scripts/c3chart.js"></script>
